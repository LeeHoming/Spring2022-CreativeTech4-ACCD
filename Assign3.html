<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dear Letter</title>
    <style>
        html {
            background-color: rgba(185, 185, 185, 0.486);
        }
    </style>
    <style type="text/css">
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            font-size: 20px;
        }
    </style>
    <style>
        .main {
            border-radius: 2px;
            width: 1200px;
        }
    </style>
</head>

<body style="text-align:center;">
    <p><a href="index.html">Home page</a></p>
    <h1>Dear letter</h1>
    <iframe width="1080" height="720" src="https://www.youtube.com/embed/PymbI6s_2C0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    <h2>Background</h2>
    <p>What makes a creature?</p>
    <p><dfn>Creature often refers to:Animal - Wikipedia</dfn></p>
    <p>How can the electron product be called biological? According to <a href="https://connectedcat.github.io/AC-CreativeTechnology3/seminar/Valentino%20Braitenberg%20Vehicles,%20Chps%201-5.pdf"><em>Vehicles:
                Experiments in Synthetic Psychology</em></a>, humans seem to be good at anthropomorphizing any stress response, meaning that simple feedback to the outside world can also give the little ESP-32 robot a certain personality. In the Project
        <em>SYNTHETIC RELATIONSHIPS</em> we were asked to construct two electronic beings capable of percieving some aspects of the environment around them. Use whatever sensors are necessary to create their <em>"worldview"</em>. Then need to connect
        them into a network, Adafruit IO, that would enable them to communicate their findings, and respond to the communications of their counterpart.</p>
    <p>Zhiyan and I started from the character of the machine to conceive a behavior pattern specific to ESP-32, to consider what capabilities to give - different sensors - and to write a language, a cryptopunk of the machine, that would allow ESP-32 to
        understand and describe the world of experience in its own way.</p>

    <h2>Agent</h2>

    <img width=800 src="image\A3\44.jpg" alt="Oh shit picture lost">

    <p>Here's a little pair of Creatures that we create! They ought to have there own idea, own way of processing information.
    </p>
    <p>In long distance relationship, they keep sending Love Massages to each other in every 20 seconds. Agent 1, who has a microphone sensor, will evaluate the surrounding sound level - the way he perceives the world, in 5 level, and reports the information
        to the cloud, which is a cloud services that they can exchange info.</p>
    <p>Agent 2, she use a PIR sensor to detect the world, too. If someone passes around, she will perceive the stimulus and the higher the intensity of the stimulus, the higher the level of information she will upload.</p>
    <p>In a relationship, we don't stay by each other's side all the time. Agent 1 and Agent 2, too, respond to the highest-ranking message, rank S, and also go back to previous messages, from rank N, to rank C, to rank B, to rank A.</p>
    <p>The next thing they do is what they do best - they try to understand and recreate the scenario that the other person is experiencing.</p>
    <img width=1200 src="image\A3\Group 1-9.png" alt="Oh shit picture lost">
    <h2>Processing information</h2>
    <p>
        <ul>
            <li>Each agent collect and exchange information. One collects motion using PIR, one collects sound using microphone. In each 20 seconds, they analyze the information and rank the stimuli for upload.</li>
            <li>Agents will receive information from IO, but just storage them without 'Read'. Once there is an extreme impact comes, agents will start to 'Read' all the information they receive.</li>
            <li>In the 'Read' processing, they will understand the information from peer in their own way. Microphone agent will express its understanding by LED array, and PIR agent will express via Buzzer - similar to the way that their pair collecting
                data, but not a simple retelling.</li>
        </ul>
    </p>
    <img width=1200 src="image\A3\up1.png" alt="Oh shit picture lost">
    <p></p>
    <embed height="100" width="120" src="image\A3\ArtCenter College of Design.m4a" />
    <p></p>


</body>

</html>